{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46ee6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install sentence-transformers\n",
    "#!pip install faiss-cpu\n",
    "#!pip install bs4\n",
    "#!pip install replicate\n",
    "#!pip install langchain_community\n",
    "#!pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5590c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "#from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import bs4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96641af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fac7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "def llama3(prompt):\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e453f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09bc2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3\", temperature = 0)\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, \n",
    "                                                   vectorstore.as_retriever(), \n",
    "                                                   return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcc061fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the main changes and features introduced by Meta's Llama 3 are:\n",
      "\n",
      "1. Four new open LLM models based on the Llama 2 architecture.\n",
      "2. Two sizes: 8B and 70B parameters, each with base (pre-trained) and instruct-tuned versions.\n",
      "3. All variants can be run on various types of consumer hardware.\n",
      "4. Context length of 8K tokens.\n",
      "\n",
      "Additionally, a new tokenizer is introduced that expands the vocabulary size to 128,256 (from 32K tokens in Llama 2), which allows for more efficient encoding of text and potentially stronger multilingualism.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What's new with Llama 3?\"\n",
    "result = chat_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2216588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Article from NTSB about crashes\n",
    "loader = WebBaseLoader([\"https://www.flyingmag.com/news/ntsb-releases-details-on-2-lockheed-12a-crashes/\"])\n",
    "#Test file - What's new about Llama 3\n",
    "loader2 = WebBaseLoader([\"https://huggingface.co/blog/llama3\"])\n",
    "docs = loader.load()\n",
    "docs2 = loader2.load()\n",
    "# Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#Loading multiple documents\n",
    "all_splits = text_splitter.split_documents(docs + docs2)\n",
    "\n",
    "\n",
    "# Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "667c92b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, on June 15 in Chino, California, a Lockheed 12A aircraft (N93R) crashed during takeoff while participating in a Father's Day airshow. The pilot and copilot were killed in the accident. The NTSB investigation found that the tailwheel lock control lever was in the locked position, but the locking tab on the control-wheel assembly appeared to be unlocked. Additionally, there was no fluid leakage observed.\n"
     ]
    }
   ],
   "source": [
    "#RAG from NTSB article\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, \n",
    "                                                   vectorstore.as_retriever(), \n",
    "                                                   return_source_documents=True)\n",
    "chat_history = []\n",
    "question = \"Can you tell me about the Chino, California incident?\"\n",
    "result = chat_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7457b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the main changes and features introduced by Meta's Llama 3 are:\n",
      "\n",
      "1. Four new open LLM models based on the Llama 2 architecture.\n",
      "2. Two sizes: 8B and 70B parameters, each with base (pre-trained) and instruct-tuned versions.\n",
      "3. All variants can be run on various types of consumer hardware.\n",
      "4. Context length of 8K tokens.\n",
      "\n",
      "Additionally, a new tokenizer is introduced that expands the vocabulary size to 128,256 (from 32K tokens in Llama 2), which allows for more efficient encoding of text and potentially stronger multilingualism.\n"
     ]
    }
   ],
   "source": [
    "#RAG from test document\n",
    "chat_history = []\n",
    "question = \"What's new with Llama 3?\"\n",
    "result = chat_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c732a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860b0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
